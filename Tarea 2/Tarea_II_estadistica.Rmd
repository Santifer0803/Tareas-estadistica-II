---
title: "Tarea_II_estadistica"
author: "Alejandro Brenes (C21319), Santiago Fernández (C22943), Eyeri Méndez (C24765)"
date: "`r Sys.Date()`"
output: html_document
---

# 1)

```{r error_aproximacion}
set.seed(12345)

# Definimos la función a integrar
g <- Vectorize(function(x)
  exp(-x ^ 2) / (1 + x ^ 2))

# Calculamos el valor exacto de la integral
int_exacta <- integrate(g, 0, 1)$value

# Tamaño de la muestra
n <- 10 ^ 6

# Generamos una m.a.s de U[0, 1]
U <- runif(n)

# Calculamos los valores de la función en los valores generados
Y <- g(U)

# Calculamos la aproximación de la integral mediante Monte Carlo
int_aprox <- mean(Y)

# Calculamos el error absoluto entre el valor aproximado y el valor exacto de la integral
error_1 <- abs(int_aprox - int_exacta)

error_1
```

En el siguiente gráfico podemos apreciar la convergencia de la función $\frac{1}{n} \sum g(u_i)$.

```{r convergencia_MonteCarlo}
prod_acum <- cumsum(Y) / (1:n)

plot(
  (1:n) / 1000,
  prod_acum,
  col = "blue",
  type = "l",
  ylab = "Aproximación",
  xlab = "Iteraciones (miles)"
)

abline(h = integrate(g, 0, 1)$value,
       col = "red",
       lwd = 1)
```


# 2)

## a)

Dado que la pérdida $L \sim Exp(\lambda)$, con $\lambda = 1$, entonces su
función de densidad está dada por $f_L(L) = e^{-L}$, y además $E[L] = \int_{0}^{\infty} L \cdot f_L(L) \, dL = \frac{1}{\lambda} = 1$.

```{r f_L(L)}
lambda <- 1

f_L <- function(L){
  
  return(lambda * exp(-lambda * L))
  
}
```

## b)

```{r valor_esperado}
set.seed(54321)

# Número de muestras
n <- 10 ^ 4

# Generamos las muestras de la distribución auxiliar g(L) ~ N(3, 4)
g_L <- rnorm(n, mean = 3, sd = 2)

# Filtramos para asegurarnos de que las muestras sean positivas
g_L <- g_L[g_L > 0]

n <- length(g_L)

# Definimos una función que calcula el valor correspondiente para cada L
f <- Vectorize(function(L)
  (L * f_L(L)) / (dnorm(L, mean = 3, sd = 2)))

# Calculamos los valores de la función en los valores de la muestra
Y <- f(g_L)

# Calculamos la estimación del valor esperado
estimacion <- mean(Y)

# Calculamos el valor esperado exacto
valor_esperado <- 1 / lambda

# Calculamos el error absoluto entre la estimación y el valor esperado
error_2 <- abs(estimacion - valor_esperado)

# Guardamos lo obtenido en una matriz
A <- matrix(c(estimacion, valor_esperado, error_2), ncol = 3)
colnames(A) <- c("Estimación", "Valor real", "Error absoluto")

A
```

Podemos ver la convergencia en el siguiente gráfico.

```{r convergencia_muestreo}
prod_acum <- cumsum(Y) / (1:n)

plot(
  (1:n) / 1000,
  prod_acum,
  col = "green",
  type = "l",
  ylab = "Aproximación",
  xlab = "Iteraciones (miles)"
)

abline(h = valor_esperado,
       col = "skyblue",
       lwd = 1)
```


# 3)

Inicialmente, agregamos la muestra que nos da el problema.

```{r muestra}
muestra <- c(2.72, 1.93, 1.76, 0.49, 6.12, 0.43, 4.01, 1.71, 2.01, 5.96)
```

## a)

Debido a que tenemos una muestra, se debe calcular su función de verosimilitud.

```{r verosimilitud}
lik <- Vectorize(function(lambda)
  prod(dexp(muestra, rate = lambda)))
```

Para la constante *c* óptima, según la muestra, se maximiza numéricamente la verosimilitud.

```{r maximo_verosim}
# Se maximiza la función
emv <- optimize(lik , int = range(muestra), maximum = TRUE)

# Se extrae la constante c
c <- emv$objective
```

Para asegurarnos que el algoritmo será eficaz, considere el siguiente gráfico.

```{r grafico_fun}
f.cuasi <- function(lambda)
  lik(lambda) * dgamma(lambda, shape = 2, scale = 1)
curve(
  c * dgamma(x, shape = 2, scale = 1),
  xlim = c(0, 4),
  ylim = c(0, c / 2),
  lty = 2,
  xlab = "mu",
  ylab = "cuasi-densidad"
)
curve(f.cuasi, add = TRUE)
```

Aplicando el algoritmo de aceptación-rechazo.

```{r algoritmo_ar}
# Cantidad de simulaciones
nsim <- 10 ^ 4

# Muestras correspondientes
U <- runif(nsim)
rc <- rgamma(nsim, shape = 2, scale = 1)

# Núumero de generaciones inicial
ngen <- length(rc)

# Verosimilitud de la muestra de gamma
Ver <- lik(rc)

# Ciclo para realizar las simulaciones requieridas
for (i in 1:nsim) {
  # Ciclo para el algoritmo de aceptación-rechazo
  while ((U[i] * c) > (Ver[i])) {
    U[i] <- runif(1)
    rc[i] <- rgamma(1, shape = 2, scale = 1)
    Ver[i] <- lik(rc[i])
    ngen <- ngen + 1
  }
}
```

El estimador de máxima verosimilitud (estimación bayesiana) de una distribución exponencial es el promedio, por lo cual, con los resultados del algoritmo anterior obtenemos el valor estimado de $\lambda$.

```{r lambda_estimado}
cat("Lambda estimado = ", mean(rc))
```

## b)

El histograma es el siguiente.

```{r histograma}
hist(rc,
     freq = FALSE,
     main = "Histograma del parámetro lambda",
     breaks = "FD")
```

## c)

```{r generaciones}
{
  cat("Número de generaciones = ", ngen)
  cat("\nNúmero medio de generaciones = ", ngen / nsim)
  cat("\nProporción de rechazos = ", 1 - (nsim / ngen), "\n")
}
```

## d)

El intervalo de credibilidad puede construirse de múltiples maneras para que acumule el $\%99$ de masa, en este caso se toma un intervalo "centrado".

```{r IC}
quantile(rc, c(0.005, 0.995))
```

## e)

En el caso del intervalo de credibilidad anterior, el valor $0.5$ se encuentra dentro del intervalo, es decir, se aceptaría la hipótesis de que $\lambda = 0.5$.

# 4)

En primer lugar, hace falta definir la función correspondiente.

```{r fun_min}
f <- function(x){(sin(10 * x)) / (10 * cos(x))}
```

## a)

Seguidamente se tiene el gráfico de la función, en el cual se puede ver que la función tiene múltiples mínimos en el intervalo requerido.

```{r grafico}
curve(f, col = "violet", lwd = 2, from = 0, to = 10, n = 1000, ylab="f(x)")
```

El algoritmo de recalentamiento simulado es el siguiente.

```{r fun_recalentamiento}
resim <- function(f,
                  alpha = 0.5,
                  s0 = 0,
                  niter,
                  mini = -Inf,
                  maxi = Inf) {
  # Valor inicial
  s_n <- s0
  
  # Vector para guardar los estados
  estados <- rep(0, niter)
  
  # Contador de iteraciones
  iter_count <- 0
  
  # Ciclo para encontrar el mínimo
  for (k in 1:niter) {
    # Estado de esta iteración
    estados[k] <- s_n
    
    # Se reduce el T según el número de iteraciones
    t <- (1 - alpha) ^ k
    
    # Nuevo estado para comparar con el actual
    s_new <- rnorm(1, s_n, 1)
    
    # Se acorta el intervalo según los estados
    if (s_new < mini) {
      s_new <- mini
    }
    if (s_new > maxi) {
      s_new <- maxi
    }
    
    # Se define la diferencia de los estados, evaluados en sus funciones
    dif <- f(s_new) - f(s_n)
    
    # Se actualiza el estado según la condición
    if (dif < 0) {
      s_n <- s_new
    }
    else {
      # Número aleatorio para comparar
      random <- runif(1, 0, 1)
      
      # Se compara el aleatorio con p
      if (random < exp(-dif / t)) {
        s_n <- s_new
      }
    }
    
    # Aumenta el contador de iteraciones
    iter_count <- iter_count + 1
  }
  return(list(minimo = s_n, estados = estados))
}
```

Se estima el mínimo usando el algoritmo anterior.

```{r minimo}
Resultado <-resim(f, 0.1, 5, 1000, 0, 10)
cat("El mínimo de la función f en [0, 10] es: ", Resultado$minimo)
```

## b)

Los estados se pueden observar en el siguiente gráfico.

```{r estados}
plot(Resultado$estados)
```


# 5)

## a)



## b)



## c)



## d)



## e)



## f)



